import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.clustering.KMeans

val raw_data = sc.textFile("/user/anagha.sebastian24_gmail/Project 2_Dataset") 
case class loudacre(dt : String, place :String, base :String, lat :Double, lon :Double)
val loudacreDF = raw_data.map(line=>line.split(",")).map(x => loudacre(x(0),x(1),x(2),x(3).toDouble,x(4).toDouble)).toDF()
val featurecols = Array("lat","lon")
val assembler = new VectorAssembler().setInputCols(featurecols).setOutputCol("features")
val df2 = assembler.transform(loudacreDF)
val Array(tarinData,testData) = df2.randomSplit(Array(0.7,0.3),5043)
val kmeans = new KMeans().setK(20).setFeaturesCol("features").setMaxIter(20)
val model = kmeans.fit(tarinData)
model.clusterCenters.foreach(println)
val categories = model.transform(testData)
categories.show()
categories.registerTempTable("loudacret")
sqlContext.sql("SELECT * FROM loudacret").show;
